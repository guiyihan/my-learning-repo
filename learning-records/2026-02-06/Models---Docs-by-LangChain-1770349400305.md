# Models - Docs by LangChain

> ğŸ“… è®°å½•æ—¶é—´: 2026/2/6 11:43:20
> ğŸ”— æ¥æº: [Models - Docs by LangChain](https://docs.langchain.com/oss/python/langchain/models)

## AIæ€»ç»“

# ç½‘é¡µå†…å®¹åˆ†ææ€»ç»“

## 1. æ ¸å¿ƒä¸»é¢˜

è¿™æ˜¯ **LangChain æ¡†æ¶ä¸­æ¨¡å‹ï¼ˆModelsï¼‰ç»„ä»¶çš„å®˜æ–¹æ–‡æ¡£**ï¼Œä¸»è¦ä»‹ç»å¦‚ä½•åœ¨ LangChain ä¸­ä½¿ç”¨å’Œé›†æˆå„ç§å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼ŒåŒ…æ‹¬æ¨¡å‹çš„åˆå§‹åŒ–ã€è°ƒç”¨æ–¹æ³•ã€å·¥å…·è°ƒç”¨ç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚

---

## 2. å…³é”®è¦ç‚¹

### â­ **æ¨¡å‹çš„æ ¸å¿ƒèƒ½åŠ›**
- **æ–‡æœ¬ç”Ÿæˆ**ï¼šå†…å®¹åˆ›ä½œã€ç¿»è¯‘ã€æ‘˜è¦ã€é—®ç­”
- **å·¥å…·è°ƒç”¨**ï¼šè°ƒç”¨å¤–éƒ¨å·¥å…·ï¼ˆæ•°æ®åº“æŸ¥è¯¢ã€APIè°ƒç”¨ï¼‰
- **ç»“æ„åŒ–è¾“å‡º**ï¼šæŒ‰é¢„å®šä¹‰æ ¼å¼ç”Ÿæˆå“åº”
- **å¤šæ¨¡æ€æ”¯æŒ**ï¼šå¤„ç†å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘
- **æ¨ç†èƒ½åŠ›**ï¼šå¤šæ­¥éª¤æ¨ç†å¾—å‡ºç»“è®º

### â­ **ä¸¤ç§ä½¿ç”¨åœºæ™¯**
- **ä¸ Agent é…åˆä½¿ç”¨**ï¼šæ¨¡å‹ä½œä¸º Agent çš„æ¨ç†å¼•æ“ï¼Œé©±åŠ¨å†³ç­–è¿‡ç¨‹
- **ç‹¬ç«‹ä½¿ç”¨**ï¼šç›´æ¥è°ƒç”¨æ¨¡å‹å®Œæˆæ–‡æœ¬ç”Ÿæˆã€åˆ†ç±»ã€æå–ç­‰ä»»åŠ¡

### â­ **ä¸‰ç§ä¸»è¦è°ƒç”¨æ–¹æ³•**
1. **Invoke**ï¼šåŒæ­¥è°ƒç”¨ï¼Œè¿”å›å®Œæ•´å“åº”
2. **Stream**ï¼šæµå¼è¾“å‡ºï¼Œå®æ—¶æ˜¾ç¤ºç”Ÿæˆå†…å®¹
3. **Batch**ï¼šæ‰¹é‡å¤„ç†å¤šä¸ªè¯·æ±‚ï¼Œæé«˜æ•ˆç‡

### â­ **ç»Ÿä¸€çš„æ¨¡å‹æ¥å£**
- æ”¯æŒæ‰€æœ‰ä¸»æµæä¾›å•†ï¼ˆOpenAIã€Anthropicã€Googleã€AWS Bedrock ç­‰ï¼‰
- é€šè¿‡ `init_chat_model` è½»æ¾åˆå§‹åŒ–å’Œåˆ‡æ¢ä¸åŒæ¨¡å‹
- æ ‡å‡†åŒ–çš„å‚æ•°é…ç½®ï¼ˆtemperatureã€max_tokensã€timeout ç­‰ï¼‰

### â­ **å·¥å…·è°ƒç”¨ï¼ˆTool Callingï¼‰**
- é€šè¿‡ `bind_tools()` ç»‘å®šè‡ªå®šä¹‰å·¥å…·
- æ¨¡å‹å¯ä»¥è¯·æ±‚æ‰§è¡Œå·¥å…·å¹¶ä½¿ç”¨ç»“æœ
- æ”¯æŒå¼ºåˆ¶å·¥å…·è°ƒç”¨å’Œå¹¶è¡Œå·¥å…·è°ƒç”¨

---

## 3. é‡è¦ç»†èŠ‚

### ğŸ“Œ **æ¨¡å‹åˆå§‹åŒ–ç¤ºä¾‹**
```python
from langchain.chat_models import init_chat_model
model = init_chat_model("gpt-4.1")
response = model.invoke("Why do parrots talk?")
```

### ğŸ“Œ **æ¶ˆæ¯æ ¼å¼**
- æ”¯æŒå­—å…¸æ ¼å¼å’Œæ¶ˆæ¯å¯¹è±¡æ ¼å¼
- æ¶ˆæ¯åŒ…å«è§’è‰²ï¼šsystemï¼ˆç³»ç»Ÿï¼‰ã€userï¼ˆç”¨æˆ·ï¼‰ã€assistantï¼ˆåŠ©æ‰‹ï¼‰
- å¯ä¼ é€’å¯¹è¯å†å²è¿›è¡Œä¸Šä¸‹æ–‡ç†è§£

### ğŸ“Œ **æµå¼è¾“å‡ºæœºåˆ¶**
- `stream()` è¿”å›è¿­ä»£å™¨ï¼Œé€å—ç”Ÿæˆå†…å®¹
- è¿”å› `AIMessageChunk` å¯¹è±¡ï¼Œå¯é€šè¿‡ç›¸åŠ èšåˆä¸ºå®Œæ•´æ¶ˆæ¯
- æ”¯æŒè‡ªåŠ¨æµå¼ä¼ è¾“ï¼ˆauto-streamingï¼‰

### ğŸ“Œ **æ‰¹å¤„ç†ä¼˜åŒ–**
- `batch()` å¹¶è¡Œå¤„ç†å¤šä¸ªè¯·æ±‚
- `batch_as_completed()` å¯æŒ‰å®Œæˆé¡ºåºè¿”å›ç»“æœ
- é€šè¿‡ `max_concurrency` æ§åˆ¶å¹¶å‘æ•°

### ğŸ“Œ **å·¥å…·è°ƒç”¨æµç¨‹**
1. ä½¿ç”¨ `@tool` è£…é¥°å™¨å®šä¹‰å·¥å…·
2. é€šè¿‡ `bind_tools()` ç»‘å®šåˆ°æ¨¡å‹
3. æ¨¡å‹è¿”å› `tool_calls` è¯·æ±‚
4. æ‰§è¡Œå·¥å…·å¹¶è¿”å›ç»“æœ
5. å°†ç»“æœä¼ å›æ¨¡å‹ç”Ÿæˆæœ€ç»ˆå“åº”

### ğŸ“Œ **æ³¨æ„äº‹é¡¹**
- ç¡®ä¿ä½¿ç”¨èŠå¤©æ¨¡å‹ï¼ˆChat å‰ç¼€ï¼‰ï¼Œè€Œéæ—§ç‰ˆ LLMï¼ˆè¿”å›å­—ç¬¦ä¸²ï¼‰
- æµå¼ä¼ è¾“éœ€è¦æ•´ä¸ªç¨‹åºæ ˆéƒ½æ”¯æŒæµå¼å¤„ç†
- å·¥å…·æ‰§è¡Œåœ¨é Agent åœºæ™¯éœ€æ‰‹åŠ¨å¤„ç†

---

## 4. ä¸ªäººå­¦ä¹ ä»·å€¼

### ğŸ’¡ **å®ç”¨æ€§ä»·å€¼**
- **å¿«é€Ÿä¸Šæ‰‹**ï¼šæä¾›äº†æ¸…æ™°çš„åˆå§‹åŒ–å’Œè°ƒç”¨ç¤ºä¾‹ï¼Œå¯ç›´æ¥åº”ç”¨äºé¡¹ç›®
- **çµæ´»æ€§**ï¼šç»Ÿä¸€æ¥å£æ”¯æŒå¤šæä¾›å•†åˆ‡æ¢ï¼Œé™ä½ä¾›åº”å•†é”å®šé£é™©
- **æ€§èƒ½ä¼˜åŒ–**ï¼šå­¦ä¹ åˆ°æ‰¹å¤„ç†å’Œå¹¶å‘æ§åˆ¶ç­‰æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### ğŸ’¡ **æ¶æ„è®¾è®¡å¯å‘**
- **æŠ½è±¡å±‚è®¾è®¡**ï¼šLangChain é€šè¿‡ç»Ÿä¸€æ¥å£å±è”½ä¸åŒæä¾›å•†çš„å·®å¼‚
- **æµå¼å¤„ç†æ€ç»´**ï¼šç†è§£æµå¼è¾“å‡ºåœ¨ç”¨æˆ·ä½“éªŒä¸­çš„é‡è¦æ€§
- **å·¥å…·ç¼–æ’æ¨¡å¼**ï¼šå­¦ä¹ å¦‚ä½•è®¾è®¡å¯æ‰©å±•çš„å·¥å…·è°ƒç”¨ç³»ç»Ÿ

### ğŸ’¡ **å¼€å‘å®è·µæŒ‡å¯¼**
- äº†è§£å¦‚ä½•æ ¹æ®ä»»åŠ¡é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼ˆæŒ‡ä»¤éµå¾ªã€ç»“æ„åŒ–æ¨ç†ã€ä¸Šä¸‹æ–‡çª—å£ï¼‰
- æŒæ¡å¯¹è¯å†å²ç®¡ç†å’Œä¸Šä¸‹æ–‡ä¼ é€’æŠ€å·§
- ç†è§£ Agent ä¸ç‹¬ç«‹è°ƒç”¨çš„åŒºåˆ«å’Œé€‚ç”¨åœºæ™¯

### ğŸ’¡ **è¿›é˜¶æ–¹å‘**
- æ·±å…¥ç ”ç©¶ `astream_events()` ç”¨äºå¤æ‚äº‹ä»¶æµå¤„ç†
- æ¢ç´¢å¼ºåˆ¶å·¥å…·è°ƒç”¨å’Œå¹¶è¡Œå·¥å…·è°ƒç”¨çš„é«˜çº§ç”¨æ³•
- å­¦ä¹ å¦‚ä½•å®ç°è‡ªå®šä¹‰å·¥å…·æ‰§è¡Œå¾ªç¯

---

**æ€»ç»“**ï¼šè¿™æ˜¯ä¸€ä»½éå¸¸å®ç”¨çš„æŠ€æœ¯æ–‡æ¡£ï¼Œæ—¢é€‚åˆåˆå­¦è€…å¿«é€Ÿå…¥é—¨ LangChain æ¨¡å‹è°ƒç”¨ï¼Œä¹Ÿä¸ºæœ‰ç»éªŒçš„å¼€å‘è€…æä¾›äº†æ€§èƒ½ä¼˜åŒ–å’Œé«˜çº§åŠŸèƒ½çš„å‚è€ƒã€‚å»ºè®®ç»“åˆå®é™…é¡¹ç›®è¾¹å­¦è¾¹ç»ƒï¼Œé‡ç‚¹æŒæ¡ä¸‰ç§è°ƒç”¨æ–¹æ³•å’Œå·¥å…·è°ƒç”¨æœºåˆ¶ã€‚

## åŸæ–‡æ‘˜å½•

Skip to main contentDocs by LangChain home pageOpen sourceSearch...NavigationCore componentsModelsCore componentsModelsCopy pageLLMs are powerful AI tools that can interpret and generate text like humans. Theyâ€™re versatile enough to write content, translate languages, summarize, and answer questions without needing specialized training for each task. In addition to text generation, many models support: Tool calling - calling external tools (like databases queries or API calls) and use results in their responses. Structured output - where the modelâ€™s response is constrained to follow a defined format. Multimodality - process and return data other than text, such as images, audio, and video. Reasoning - models perform multi-step reasoning to arrive at a conclusion. Models are the reasoning engine of agents. They drive the agentâ€™s decision-making process, determining which tools to call, how to interpret results, and when to provide a final answer. The quality and capabilities of the model you choose directly impact your agentâ€™s baseline reliability and performance. Different models excel at different tasks - some are better at following complex instructions, others at structured reasoning, and some support larger context windows for handling more information. LangChainâ€™s standard model interfaces give you access to many different provider integrations, which makes it easy to experiment with and switch between models to find the best fit for your use case. For provider-specific integration information and capabilities, see the providerâ€™s chat model page. â€‹Basic usage Models can be utilized in two ways: With agents - Models can be dynamically specified when creating an agent. Standalone - Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework. The same model interface works in both contexts, which gives you the flexibility to start simple and scale up to more complex agent-based workflows as needed. â€‹Initialize a model The easiest way to get started with a standalone model in LangChain is to use init_chat_model to initialize one from a chat model provider of your choice (examples below): OpenAI Anthropic Azure Google Gemini AWS Bedrock HuggingFaceğŸ‘‰ Read the OpenAI chat model integration docsCopypip install -U "langchain[openai]" init_chat_modelModel ClassCopyimport os from langchain.chat_models import init_chat_model os.environ["OPENAI_API_KEY"] = "sk-..." model = init_chat_model("gpt-4.1") Copyresponse = model.invoke("Why do parrots talk?") See init_chat_model for more detail, including information on how to pass model parameters. â€‹Supported models LangChain supports all major model providers, including OpenAI, Anthropic, Google, Azure, AWS Bedrock, and more. Each provider offers a variety of models with different capabilities. For a full list of supported models in LangChain, see the integrations page. â€‹Key methods InvokeThe model takes messages as input and outputs messages after generating a complete response. StreamInvoke the model, but stream the output as it is generated in real-time. BatchSend multiple requests to a model in a batch for more efficient processing. In addition to chat models, LangChain provides support for other adjacent technologies, such as embedding models and vector stores. See the integrations page for details. â€‹Parameters A chat model takes parameters that can be used to configure its behavior. The full set of supported parameters varies by model and provider, but standard ones include: â€‹modelstringrequiredThe name or identifier of the specific model you want to use with a provider. You can also specify both the model and its provider in a single argument using the â€™:â€™ format, for example, â€˜openai:o1â€™. â€‹api_keystringThe key required for authenticating with the modelâ€™s provider. This is usually issued when you sign up for access to the model. Often accessed by setting an environment variable. â€‹temperaturenumberControls the randomness of the modelâ€™s output. A higher number makes responses more creative; lower ones make them more deterministic. â€‹max_tokensnumberLimits the total number of tokens in the response, effectively controlling how long the output can be. â€‹timeoutnumberThe maximum time (in seconds) to wait for a response from the model before canceling the request. â€‹max_retriesnumberThe maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits. Using init_chat_model, pass these parameters as inline **kwargs: Initialize using model parametersCopymodel = init_chat_model( "claude-sonnet-4-5-20250929", # Kwargs passed to the model: temperature=0.7, timeout=30, max_tokens=1000, ) Each chat model integration may have additional params used to control provider-specific functionality.For example, ChatOpenAI has use_responses_api to dictate whether to use the OpenAI Responses or Completions API.To find all the p

[å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­...]

---

*ç”±æ™ºèƒ½å­¦ä¹ åŠ©æ‰‹è‡ªåŠ¨ç”Ÿæˆ*
